# Эксперимент 20: Общий путь + разветвление SGD

## Описание
Эксперимент с общим начальным путем и последующим разветвлением на разные learning rates:

1. **Общий путь SGD** (10 эпох, lr=0.1) - общее обучение для всех веток
2. **Общий путь GD** (10 эпох, lr=0.1) - доползание до минимума
3. **Разветвление SGD** (16 эпох каждая ветка) - SGD с разными lr + логирование

## Параметры
- **Общий lr**: 0.1 (1e-1) для SGD и GD этапов
- **Ветки lr**: 0.001, 0.01, 0.1, 0.5 (1e-3, 1e-2, 1e-1, 5e-1)
- **Batch size**: 64
- **Модель**: FlexibleMLP (8 hidden, 1 layer, downsample=6)
- **Датасет**: MNIST (1000 samples)

## Запуск

### Автоматический запуск:
```bash
./run_exp20.sh
```

### Ручной запуск:
```bash
python exp20_train.py --initial-lr 0.1 --branch-lrs "0.001,0.01,0.1,0.5"
```

### Запуск через runner:
```bash
python exp20_runner.py
```

## Структура результатов

```
data/checkpoints/exp20/
├── params_lr0.001.pkl           # Траектории параметров ветки lr=1e-3
├── hessians_lr0.001.pkl         # Гессианы ветки lr=1e-3
├── params_lr0.01.pkl            # Траектории параметров ветки lr=1e-2
├── hessians_lr0.01.pkl          # Гессианы ветки lr=1e-2
├── params_lr0.1.pkl             # Траектории параметров ветки lr=1e-1
├── hessians_lr0.1.pkl           # Гессианы ветки lr=1e-1
├── params_lr0.5.pkl             # Траектории параметров ветки lr=5e-1
├── hessians_lr0.5.pkl           # Гессианы ветки lr=5e-1
├── metadata_exp20.pkl           # Общие метаданные
└── exp20_branches.png           # Объединенный график всех веток
```

## Анализ результатов

### Метаданные содержат:
- `sgd_end`: точка окончания общего SGD (в эпохах)
- `gd_end`: точка окончания общего GD (в эпохах)
- `common_train_losses`: потери общего пути
- `common_val_losses`: валидационные потери общего пути
- `branch_lrs`: список learning rates веток

### Графики показывают:
- **Общую траекторию**: SGD → GD (черная линия)
- **Разветвление**: каждая ветка своим цветом
- **Переходы**: вертикальные линии SGD→GD и GD→ветки
- **Отдельные графики**: только ветки для детального анализа

### Данные веток:
- **Параметры**: траектории по батчам для каждой ветки
- **Гессианы**: матрицы гессианов по батчам для каждой ветки
- **Размеры**: автоматический вывод размеров массивов в MB

## Цель эксперимента
Изучить, как разные learning rates влияют на поведение SGD при старте из одной и той же точки (после GD оптимизации). Это позволит:
- Сравнить стохастический шум для разных lr
- Проанализировать гессианы в зависимости от lr
- Понять влияние lr на траектории в пространстве параметров
- Визуализировать разветвление оптимизации из общей точки
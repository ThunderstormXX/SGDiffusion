import os
import sys
import torch
import numpy as np
import random
import argparse
import pickle
from tqdm import tqdm
import torch.nn as nn
import matplotlib.pyplot as plt

# ======== –õ–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥—É–ª–∏ ========
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
from src.utils import MNIST
from src.model import FlexibleMLP

def parse_args():
    parser = argparse.ArgumentParser(description='Common start then branch SGD - Experiment 20')
    parser.add_argument('--initial-lr', type=float, default=0.1, help='Initial learning rate for common path')
    parser.add_argument('--branch-lrs', type=str, default='0.001,0.01,0.1,0.5', help='Branch learning rates (comma-separated)')
    parser.add_argument('--batch-size', type=int, default=64, help='Batch size')
    parser.add_argument('--sgd-epochs', type=int, default=10, help='Initial SGD epochs')
    parser.add_argument('--gd-epochs', type=int, default=10, help='GD epochs to minimum')
    parser.add_argument('--branch-epochs', type=int, default=16, help='Branch SGD epochs with logging')
    parser.add_argument('--save-dir', type=str, default='data/checkpoints/exp20', help='Save directory')
    parser.add_argument('--seed', type=int, default=228, help='Random seed')
    return parser.parse_args()

def compute_hessian(model, images, labels, criterion, device):
    """–í—ã—á–∏—Å–ª—è–µ—Ç –≥–µ—Å—Å–∏–∞–Ω –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ –±–∞—Ç—á–∞"""
    model.eval()
    params = [p for p in model.parameters() if p.requires_grad]
    n_params = sum(p.numel() for p in params)
    
    model.zero_grad()
    outputs = model(images)
    loss = criterion(outputs, labels)
    grads = torch.autograd.grad(loss, params, create_graph=True)
    
    hessian = torch.zeros(n_params, n_params, device=device)
    param_idx = 0
    
    for grad in grads:
        grad_flat = grad.contiguous().view(-1)
        for j, g in enumerate(grad_flat):
            if g.requires_grad:
                grad2 = torch.autograd.grad(g, params, retain_graph=True, allow_unused=True)
                col_idx = 0
                for k, g2 in enumerate(grad2):
                    if g2 is not None:
                        g2_flat = g2.contiguous().view(-1)
                        hessian[param_idx + j, col_idx:col_idx + len(g2_flat)] = g2_flat
                        col_idx += len(g2_flat)
                    else:
                        col_idx += params[k].numel()
        param_idx += grad.numel()
    
    return hessian.cpu().numpy()

def get_model_params_vector(model):
    """–ü–æ–ª—É—á–∞–µ—Ç —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç—ã–π –≤–µ–∫—Ç–æ—Ä –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏"""
    params = []
    for p in model.parameters():
        params.append(p.data.view(-1))
    return torch.cat(params).cpu().numpy()

def evaluate_model(model, test_loader, criterion, device):
    """–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–π –≤—ã–±–æ—Ä–∫–µ"""
    model.eval()
    total_loss = 0
    correct = 0
    total = 0
    
    with torch.no_grad():
        for images, labels in test_loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            loss = criterion(outputs, labels)
            total_loss += loss.item()
            
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    
    avg_loss = total_loss / len(test_loader)
    accuracy = 100 * correct / total
    return avg_loss, accuracy

def main():
    args = parse_args()
    
    # –ü–∞—Ä—Å–∏–º branch learning rates
    branch_lrs = [float(lr) for lr in args.branch_lrs.split(',')]
    
    # ======== –§–∏–∫—Å–∞—Ü–∏—è —Å–ª—É—á–∞–π–Ω–æ—Å—Ç–∏ ========
    torch.manual_seed(args.seed)
    np.random.seed(args.seed)
    random.seed(args.seed)
    
    # ======== –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ========
    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
    SAMPLE_SIZE = 1000
    
    os.makedirs(args.save_dir, exist_ok=True)
    
    # ======== –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö ========
    train_dataset, test_dataset, train_loader, test_loader = MNIST(
        batch_size=args.batch_size, sample_size=SAMPLE_SIZE
    )
    
    # –°–æ–∑–¥–∞–µ–º –ø–æ–ª–Ω—ã–π –±–∞—Ç—á –¥–ª—è GD
    all_images = []
    all_labels = []
    for images, labels in train_loader:
        all_images.append(images)
        all_labels.append(labels)
    full_batch_images = torch.cat(all_images, dim=0).to(DEVICE)
    full_batch_labels = torch.cat(all_labels, dim=0).to(DEVICE)
    
    # ======== –ú–æ–¥–µ–ª—å ========
    model = FlexibleMLP(
        hidden_dim=8,
        num_hidden_layers=1,
        input_downsample=6
    ).to(DEVICE)
    
    criterion = nn.CrossEntropyLoss()
    
    print(f"\nüìä –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ 20:")
    print(f"   Device: {DEVICE}")
    print(f"   Initial learning rate: {args.initial_lr}")
    print(f"   Branch learning rates: {branch_lrs}")
    print(f"   Batch size: {args.batch_size}")
    print(f"   Sample size: {SAMPLE_SIZE}")
    print(f"   SGD epochs: {args.sgd_epochs}")
    print(f"   GD epochs: {args.gd_epochs}")
    print(f"   Branch epochs: {args.branch_epochs}")
    
    # –•—Ä–∞–Ω–∏–ª–∏—â–∞ –¥–ª—è –æ–±—â–µ–≥–æ –ø—É—Ç–∏
    common_train_losses = []
    common_val_losses = []
    common_val_accuracies = []
    
    # ======== –û–ë–©–ò–ô –ü–£–¢–¨: SGD + GD ========
    print(f"\nüöÄ –û–ë–©–ò–ô –ü–£–¢–¨: SGD ({args.sgd_epochs} —ç–ø–æ—Ö) + GD ({args.gd_epochs} —ç–ø–æ—Ö)...")
    
    # SGD —ç—Ç–∞–ø
    optimizer_sgd = torch.optim.SGD(model.parameters(), lr=args.initial_lr)
    model.train()
    
    pbar = tqdm(range(args.sgd_epochs), desc="Common SGD", ncols=100)
    for epoch in pbar:
        epoch_losses = []
        for images, labels in train_loader:
            images, labels = images.to(DEVICE), labels.to(DEVICE)
            
            optimizer_sgd.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer_sgd.step()
            
            epoch_losses.append(loss.item())
        
        epoch_avg_loss = np.mean(epoch_losses)
        common_train_losses.append(epoch_avg_loss)
        
        val_loss, val_acc = evaluate_model(model, test_loader, criterion, DEVICE)
        common_val_losses.append(val_loss)
        common_val_accuracies.append(val_acc)
        
        pbar.set_postfix({
            'train_loss': f'{epoch_avg_loss:.4f}',
            'val_loss': f'{val_loss:.4f}',
            'val_acc': f'{val_acc:.1f}%'
        })
    
    sgd_end = len(common_train_losses)
    
    # GD —ç—Ç–∞–ø
    optimizer_gd = torch.optim.SGD(model.parameters(), lr=args.initial_lr)
    model.train()
    
    pbar = tqdm(range(args.gd_epochs), desc="Common GD", ncols=100)
    for epoch in pbar:
        optimizer_gd.zero_grad()
        outputs = model(full_batch_images)
        loss = criterion(outputs, full_batch_labels)
        loss.backward()
        optimizer_gd.step()
        
        common_train_losses.append(loss.item())
        
        val_loss, val_acc = evaluate_model(model, test_loader, criterion, DEVICE)
        common_val_losses.append(val_loss)
        common_val_accuracies.append(val_acc)
        
        pbar.set_postfix({
            'train_loss': f'{loss.item():.4f}',
            'val_loss': f'{val_loss:.4f}',
            'val_acc': f'{val_acc:.1f}%'
        })
    
    gd_end = len(common_train_losses)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —Ä–∞–∑–≤–µ—Ç–≤–ª–µ–Ω–∏—è
    branch_start_state = model.state_dict().copy()
    
    print(f"‚úÖ –û–±—â–∏–π –ø—É—Ç—å –∑–∞–≤–µ—Ä—à–µ–Ω. SGD: {sgd_end} —ç–ø–æ—Ö, GD: {gd_end} —ç–ø–æ—Ö")
    
    # ======== –†–ê–ó–í–ï–¢–í–õ–ï–ù–ò–ï –î–õ–Ø –†–ê–ó–ù–´–• LR ========
    all_branch_data = {}
    
    for branch_lr in branch_lrs:
        print(f"\nüåø –í–ï–¢–ö–ê lr={branch_lr}: SGD —Å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º ({args.branch_epochs} —ç–ø–æ—Ö)...")
        
        # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ –º–æ–¥–µ–ª–∏
        model.load_state_dict(branch_start_state)
        
        optimizer_branch = torch.optim.SGD(model.parameters(), lr=branch_lr)
        
        branch_train_losses = []
        branch_val_losses = []
        branch_val_accuracies = []
        params_vectors = []
        hessians = []
        
        pbar = tqdm(range(args.branch_epochs), desc=f"Branch lr={branch_lr}", ncols=100)
        
        for epoch in pbar:
            epoch_losses = []
            epoch_hessians = []
            epoch_params = []
            
            for images, labels in train_loader:
                images, labels = images.to(DEVICE), labels.to(DEVICE)
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–æ —à–∞–≥–∞
                params_vector = get_model_params_vector(model)
                epoch_params.append(params_vector)
                
                # –í—ã—á–∏—Å–ª—è–µ–º loss
                model.train()
                outputs = model(images)
                loss = criterion(outputs, labels)
                epoch_losses.append(loss.item())
                
                # –í—ã—á–∏—Å–ª—è–µ–º –≥–µ—Å—Å–∏–∞–Ω
                hessian = compute_hessian(model, images, labels, criterion, DEVICE)
                epoch_hessians.append(hessian)
                
                # –î–µ–ª–∞–µ–º —à–∞–≥ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
                optimizer_branch.zero_grad()
                loss.backward()
                optimizer_branch.step()
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ –±–∞—Ç—á–∏ —ç–ø–æ—Ö–∏
            params_vectors.extend(epoch_params)
            hessians.extend(epoch_hessians)
            
            # –õ–æ–≥–∏—Ä—É–µ–º —Å—Ä–µ–¥–Ω–∏–π –ª–æ—Å—Å –∑–∞ —ç–ø–æ—Ö—É
            epoch_avg_loss = np.mean(epoch_losses)
            branch_train_losses.append(epoch_avg_loss)
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–∞–∂–¥—É—é —ç–ø–æ—Ö—É
            val_loss, val_acc = evaluate_model(model, test_loader, criterion, DEVICE)
            branch_val_losses.append(val_loss)
            branch_val_accuracies.append(val_acc)
            
            pbar.set_postfix({
                'train_loss': f'{epoch_avg_loss:.4f}',
                'val_loss': f'{val_loss:.4f}',
                'val_acc': f'{val_acc:.1f}%'
            })
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –≤–µ—Ç–∫–∏
        all_branch_data[branch_lr] = {
            'train_losses': branch_train_losses,
            'val_losses': branch_val_losses,
            'val_accuracies': branch_val_accuracies,
            'params_vectors': params_vectors,
            'hessians': hessians
        }
        
        print(f"‚úÖ –í–µ—Ç–∫–∞ lr={branch_lr} –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
    
    # ======== –°–æ–∑–¥–∞–Ω–∏–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω–æ–≥–æ –≥—Ä–∞—Ñ–∏–∫–∞ ========
    plt.figure(figsize=(20, 10))
    
    # –ì—Ä–∞—Ñ–∏–∫ –ø–æ—Ç–µ—Ä—å
    plt.subplot(2, 3, 1)
    # –û–±—â–∏–π –ø—É—Ç—å
    plt.plot(range(len(common_train_losses)), common_train_losses, 'k-', linewidth=2, label='Common path')
    plt.axvline(x=sgd_end, color='red', linestyle='--', alpha=0.8, label='SGD ‚Üí GD')
    plt.axvline(x=gd_end, color='blue', linestyle='--', alpha=0.8, label='GD ‚Üí Branch')
    
    # –í–µ—Ç–∫–∏
    colors = ['red', 'green', 'blue', 'orange']
    for i, (branch_lr, data) in enumerate(all_branch_data.items()):
        branch_x = range(gd_end, gd_end + len(data['train_losses']))
        plt.plot(branch_x, data['train_losses'], color=colors[i % len(colors)], 
                label=f'SGD lr={branch_lr}', alpha=0.8)
    
    plt.title('Training Loss - All Branches')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True)
    
    # –õ–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–∏–π –≥—Ä–∞—Ñ–∏–∫
    plt.subplot(2, 3, 2)
    plt.semilogy(range(len(common_train_losses)), common_train_losses, 'k-', linewidth=2, label='Common path')
    plt.axvline(x=sgd_end, color='red', linestyle='--', alpha=0.8)
    plt.axvline(x=gd_end, color='blue', linestyle='--', alpha=0.8)
    
    for i, (branch_lr, data) in enumerate(all_branch_data.items()):
        branch_x = range(gd_end, gd_end + len(data['train_losses']))
        plt.semilogy(branch_x, data['train_losses'], color=colors[i % len(colors)], 
                    label=f'SGD lr={branch_lr}', alpha=0.8)
    
    plt.title('Training Loss (log scale)')
    plt.xlabel('Epoch')
    plt.ylabel('Loss (log)')
    plt.legend()
    plt.grid(True)
    
    # –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã–µ –ø–æ—Ç–µ—Ä–∏
    plt.subplot(2, 3, 3)
    plt.plot(range(len(common_val_losses)), common_val_losses, 'k-', linewidth=2, marker='o', label='Common path')
    plt.axvline(x=sgd_end, color='red', linestyle='--', alpha=0.8)
    plt.axvline(x=gd_end, color='blue', linestyle='--', alpha=0.8)
    
    for i, (branch_lr, data) in enumerate(all_branch_data.items()):
        branch_x = range(gd_end, gd_end + len(data['val_losses']))
        plt.plot(branch_x, data['val_losses'], color=colors[i % len(colors)], 
                marker='s', label=f'SGD lr={branch_lr}', alpha=0.8)
    
    plt.title('Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True)
    
    # –í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å
    plt.subplot(2, 3, 4)
    plt.plot(range(len(common_val_accuracies)), common_val_accuracies, 'k-', linewidth=2, marker='o', label='Common path')
    plt.axvline(x=sgd_end, color='red', linestyle='--', alpha=0.8)
    plt.axvline(x=gd_end, color='blue', linestyle='--', alpha=0.8)
    
    for i, (branch_lr, data) in enumerate(all_branch_data.items()):
        branch_x = range(gd_end, gd_end + len(data['val_accuracies']))
        plt.plot(branch_x, data['val_accuracies'], color=colors[i % len(colors)], 
                marker='s', label=f'SGD lr={branch_lr}', alpha=0.8)
    
    plt.title('Validation Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy (%)')
    plt.legend()
    plt.grid(True)
    
    # –¢–æ–ª—å–∫–æ –≤–µ—Ç–∫–∏ - –ø–æ—Ç–µ—Ä–∏
    plt.subplot(2, 3, 5)
    for i, (branch_lr, data) in enumerate(all_branch_data.items()):
        plt.plot(data['train_losses'], color=colors[i % len(colors)], 
                label=f'lr={branch_lr}', alpha=0.8)
    
    plt.title('Branch Training Losses Only')
    plt.xlabel('Branch Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True)
    
    # –¢–æ–ª—å–∫–æ –≤–µ—Ç–∫–∏ - —Ç–æ—á–Ω–æ—Å—Ç—å
    plt.subplot(2, 3, 6)
    for i, (branch_lr, data) in enumerate(all_branch_data.items()):
        plt.plot(data['val_accuracies'], color=colors[i % len(colors)], 
                marker='o', label=f'lr={branch_lr}', alpha=0.8)
    
    plt.title('Branch Validation Accuracy Only')
    plt.xlabel('Branch Epoch')
    plt.ylabel('Accuracy (%)')
    plt.legend()
    plt.grid(True)
    
    plt.tight_layout()
    plot_path = os.path.join(args.save_dir, f'exp20_branches.png')
    plt.savefig(plot_path)
    plt.close()
    
    # ======== –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ ========
    for branch_lr, data in all_branch_data.items():
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∏ –≥–µ—Å—Å–∏–∞–Ω—ã
        params_tensor = np.array(data['params_vectors'])
        hessians_tensor = np.array(data['hessians'])
        
        params_path = os.path.join(args.save_dir, f"params_lr{branch_lr}.pkl")
        hessians_path = os.path.join(args.save_dir, f"hessians_lr{branch_lr}.pkl")
        
        with open(params_path, 'wb') as f:
            pickle.dump(params_tensor, f)
        
        with open(hessians_path, 'wb') as f:
            pickle.dump(hessians_tensor, f)
        
        print(f"\nüíæ –í–µ—Ç–∫–∞ lr={branch_lr}:")
        print(f"   –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: {params_path}")
        print(f"   üìè –†–∞–∑–º–µ—Ä: {params_tensor.shape} ({params_tensor.nbytes / 1024 / 1024:.1f} MB)")
        print(f"   –ì–µ—Å—Å–∏–∞–Ω—ã: {hessians_path}")
        print(f"   üìè –†–∞–∑–º–µ—Ä: {hessians_tensor.shape} ({hessians_tensor.nbytes / 1024 / 1024:.1f} MB)")
    
    # –û–±—â–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å –¥–∞–Ω–Ω—ã–º–∏ –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–æ–≤
    metadata = {
        'initial_lr': args.initial_lr,
        'branch_lrs': branch_lrs,
        'batch_size': args.batch_size,
        'sgd_epochs': args.sgd_epochs,
        'gd_epochs': args.gd_epochs,
        'branch_epochs': args.branch_epochs,
        'sgd_end': sgd_end,
        'gd_end': gd_end,
        'common_train_losses': common_train_losses,
        'common_val_losses': common_val_losses,
        'common_val_accuracies': common_val_accuracies,
        'branch_data': {lr: {
            'train_losses': data['train_losses'],
            'val_losses': data['val_losses'], 
            'val_accuracies': data['val_accuracies']
        } for lr, data in all_branch_data.items()},
        'seed': args.seed,
        'sample_size': SAMPLE_SIZE
    }
    
    metadata_path = os.path.join(args.save_dir, f"metadata_exp20.pkl")
    with open(metadata_path, 'wb') as f:
        pickle.dump(metadata, f)
    
    print(f"\nüíæ –û–±—â–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ: {metadata_path}")
    print(f"üìà –ì—Ä–∞—Ñ–∏–∫: {plot_path}")
    print(f"\nüìä –°–≤–æ–¥–∫–∞:")
    print(f"   üî¢ –í–µ—Ç–æ–∫: {len(branch_lrs)}")
    print(f"   üî¢ –û–±—â–∏—Ö —ç–ø–æ—Ö: {len(common_train_losses)}")
    print(f"   üî¢ –ë–∞—Ç—á–µ–π –Ω–∞ –≤–µ—Ç–∫—É: {len(all_branch_data[branch_lrs[0]]['params_vectors'])}")

if __name__ == "__main__":
    main()